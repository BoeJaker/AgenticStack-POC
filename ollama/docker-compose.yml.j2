version: "3.9"

services:
  # CPU Ollama
  ollama-cpu:
    image: ollama/ollama:latest
    container_name: ollama-cpu
    restart: unless-stopped
    environment:
      - OLLAMA_KEEP_ALIVE=86400
      - OLLAMA_NUM_THREADS={{ CPU_CORES }}
      - OLLAMA_MODEL_PATH=/mnt/ramdisk_cpu
    volumes:
      - ollama_data_cpu:/root/.ollama
      - ramdisk_cpu:/mnt/ramdisk_cpu
      - ./preload/preload_models.py:/app/preload_models.py
    ports:
      - "11435:11434"
    command: >
      /bin/bash -c "
      python /app/preload_models.py &&
      ollama serve --port 11434
      "
    deploy:
      resources:
        limits:
          cpus: "{{ CPU_CORES }}"
          memory: "{{ CPU_MEMORY }}"

{% for gpu_name, gpu_id in gpu_list %}
{% for idx in range(gpu_instances_per_gpu) %}
  ollama-{{ gpu_name }}-{{ idx }}:
    image: ollama/ollama:latest
    container_name: ollama-{{ gpu_name }}-{{ idx }}
    restart: unless-stopped
    environment:
      - OLLAMA_KEEP_ALIVE=99999999
      - OLLAMA_MODEL_PATH=/mnt/ramdisk_{{ gpu_name }}_{{ idx }}
      - NVIDIA_VISIBLE_DEVICES={{ gpu_id }}
    volumes:
      - ollama_data_{{ gpu_name }}_{{ idx }}:/root/.ollama
      - ramdisk_{{ gpu_name }}_{{ idx }}:/mnt/ramdisk_{{ gpu_name }}_{{ idx }}
      - ./preload/preload_models.py:/app/preload_models.py
    ports:
      - "{{ 11436 + loop.index0 + (idx * 10) }}:11434"
    runtime: nvidia
    command: >
      /bin/bash -c "
      python /app/preload_models.py &&
      ollama serve --port 11434
      "
{% endfor %}
{% endfor %}

  # Router
  ollama-router:
    build: ./router
    container_name: ollama-router
    restart: unless-stopped
    environment:
      - GPU_NAMES={{ ",".join(gpu_names) }}
      - GPU_IDS={{ ",".join(gpu_ids) }}
      - PRELOAD_MODELS={{ PRELOAD_MODELS }}
    ports:
      - "11434:80"
    depends_on:
      - ollama-cpu
{% for gpu_name, gpu_id in gpu_list %}
{% for idx in range(gpu_instances_per_gpu) %}
      - ollama-{{ gpu_name }}-{{ idx }}
{% endfor %}
{% endfor %}

volumes:
  ollama_data_cpu:
{% for gpu_name, gpu_id in gpu_list %}
{% for idx in range(gpu_instances_per_gpu) %}
  ollama_data_{{ gpu_name }}_{{ idx }}:
  ramdisk_{{ gpu_name }}_{{ idx }}:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: size={{ GPU_RAMDISK_SIZE }}
{% endfor %}
{% endfor %}
  ramdisk_cpu:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: size={{ CPU_RAMDISK_SIZE }}
